{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"13f4d50697524846ba6a4b93bced718d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fceca26f4ea4634b0dc0bed312ca031","IPY_MODEL_44f64bdbb6884c07bcab4deb69e1b0ca","IPY_MODEL_d18ba62d1d674b40a3d03efd1c0a1318"],"layout":"IPY_MODEL_794f7abdf8444480942f815a57fd0361"}},"1fceca26f4ea4634b0dc0bed312ca031":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0258f3c200a14f239ec21e3837d3f374","placeholder":"​","style":"IPY_MODEL_438eeec7eeaa4d40bb95b71e652c96f8","value":"Map: 100%"}},"44f64bdbb6884c07bcab4deb69e1b0ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec92ff2e8f3544c2a0c045f26ce1d554","max":6633,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4991483b0e14235b79479709a536f7b","value":6633}},"d18ba62d1d674b40a3d03efd1c0a1318":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f0713bc90aa4d6ca275e4d85bc4234a","placeholder":"​","style":"IPY_MODEL_9ed2a08224f34e42a9a93f7cc68c5908","value":" 6633/6633 [01:23&lt;00:00, 94.64 examples/s]"}},"794f7abdf8444480942f815a57fd0361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0258f3c200a14f239ec21e3837d3f374":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"438eeec7eeaa4d40bb95b71e652c96f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec92ff2e8f3544c2a0c045f26ce1d554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4991483b0e14235b79479709a536f7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f0713bc90aa4d6ca275e4d85bc4234a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed2a08224f34e42a9a93f7cc68c5908":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5so5GwjSeX9I","outputId":"5b5883a6-1191-4203-8fee-48c27902da52","execution":{"iopub.status.busy":"2024-06-17T04:59:53.522043Z","iopub.execute_input":"2024-06-17T04:59:53.522843Z","iopub.status.idle":"2024-06-17T05:00:08.025343Z","shell.execute_reply.started":"2024-06-17T04:59:53.522808Z","shell.execute_reply":"2024-06-17T05:00:08.024281Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m930.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import library","metadata":{"id":"U-0fOIA4uc0f"}},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport numpy as np\nimport torch\nimport random\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n# a seed for reproducibility\nSEED = 42\n# set seed\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nrandom.seed(SEED)\n\n# check for GPU device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Device available:', device) ","metadata":{"id":"5WGsG3iSe0-6","execution":{"iopub.status.busy":"2024-06-17T05:00:08.027921Z","iopub.execute_input":"2024-06-17T05:00:08.028307Z","iopub.status.idle":"2024-06-17T05:00:29.807386Z","shell.execute_reply.started":"2024-06-17T05:00:08.028261Z","shell.execute_reply":"2024-06-17T05:00:29.806419Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-17 05:00:17.994402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-17 05:00:17.994535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-17 05:00:18.151255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device available: cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load model","metadata":{"id":"IOUG27YnutVv"}},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEUiY5FMuvWW","outputId":"714332a8-5ac4-4ba8-e7c4-0e3523621e57","execution":{"iopub.status.busy":"2024-06-17T05:00:29.808950Z","iopub.execute_input":"2024-06-17T05:00:29.809729Z","iopub.status.idle":"2024-06-17T05:00:35.006395Z","shell.execute_reply.started":"2024-06-17T05:00:29.809689Z","shell.execute_reply":"2024-06-17T05:00:35.003541Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54b99dfb12d46b680033d9f044f53af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001f5f1c5c8b4ccc9fb16095ac44e06c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827b9298706f4abeba47189ca779147b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8da2a1fc3f44aa08d5e8d698f0de775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64fbd8efd0d54270b43cc42e2a755841"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"My mother is a person I admire most. She devoted a lot of time and energy to the upbringing of my two brothers and 1. Despite working hard, she always made time to teach us many useful things which are necessary and important in our later lives. Moreover, she is a good role model for me to follow. She always tries to get on well with people who live next door and help everyone when they are in difficulties, so most of them respect and love her. I admire and look up to my mother because she not only brings me up well but also stands by me and gives some help if necessary. For example, when I encounter some difficulties, she will give me some precious advice to help me solve those problems. She has a major influence on me and 1 hope that I will inherit some of her traits.\"\n\n\ninputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\nsummary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=40, max_length=160)\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"EPn_6bs6vEtg","outputId":"559a78e4-1059-411d-8a9e-7f093535ce0d","execution":{"iopub.status.busy":"2024-06-17T05:00:35.008338Z","iopub.execute_input":"2024-06-17T05:00:35.008768Z","iopub.status.idle":"2024-06-17T05:00:39.748731Z","shell.execute_reply.started":"2024-06-17T05:00:35.008729Z","shell.execute_reply":"2024-06-17T05:00:39.747813Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'My mother is a person I admire most. She devoted a lot of time and energy to the upbringing of my two brothers and 1. Despite working hard, she always made time to teach us many useful things which are necessary and important in our later lives. Moreover, she is a good role model for me to follow. She always tries to get on well with people who live next door and help everyone when they are in difficulties, so most of them respect and love her. I admire and look up to my mother because she not only brings me up well but also stands by me and gives some help if necessary. For example, when I encounter some difficulties, she will give me some precious advice to help me solve those problems. She has a major influence on me and I hope that'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load dataset","metadata":{"id":"aulEw7_qvTe_"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = \"ccdv/pubmed-summarization\"\ndataset = load_dataset(dataset_name, trust_remote_code=True) #, split=\"train[:1%]\")","metadata":{"id":"zRRo6S6mvVah","execution":{"iopub.status.busy":"2024-06-17T05:00:39.751079Z","iopub.execute_input":"2024-06-17T05:00:39.751370Z","iopub.status.idle":"2024-06-17T05:01:59.461675Z","shell.execute_reply.started":"2024-06-17T05:00:39.751346Z","shell.execute_reply":"2024-06-17T05:01:59.460669Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a12cab73259d4f608ef3ad72bb914415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ca2ca24cb0488580990957ce879164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/779M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8eb7a7091a435b9b9bddd7408a21b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/43.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ce905067a94e9b97031ac274c7408a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/43.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd6437616dd4da69f53ff7c5fe50b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162c999078da478d8330106fe463ef6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc165e0f98148d5b289382845fa66cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d837e8825afa48e9b737824902f67e9f"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhKiawevxGlV","outputId":"b2b06dc1-828a-479e-e5e7-5c634338b214","execution":{"iopub.status.busy":"2024-06-17T05:01:59.463040Z","iopub.execute_input":"2024-06-17T05:01:59.463396Z","iopub.status.idle":"2024-06-17T05:01:59.471492Z","shell.execute_reply.started":"2024-06-17T05:01:59.463360Z","shell.execute_reply":"2024-06-17T05:01:59.470601Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'abstract'],\n        num_rows: 119924\n    })\n    validation: Dataset({\n        features: ['article', 'abstract'],\n        num_rows: 6633\n    })\n    test: Dataset({\n        features: ['article', 'abstract'],\n        num_rows: 6658\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# small_train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n# small_test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))","metadata":{"id":"VCH4Go8D0yza","execution":{"iopub.status.busy":"2024-06-17T05:01:59.472692Z","iopub.execute_input":"2024-06-17T05:01:59.473020Z","iopub.status.idle":"2024-06-17T05:02:02.774961Z","shell.execute_reply.started":"2024-06-17T05:01:59.472989Z","shell.execute_reply":"2024-06-17T05:02:02.773884Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n  inputs = [doc for doc in examples[\"article\"]]\n  model_inputs = tokenizer(inputs, max_length=1024, truncation=True,padding='max_length')\n  with tokenizer.as_target_tokenizer():\n    labels = tokenizer(examples[\"abstract\"], max_length=128, truncation=True, padding='max_length')\n  model_inputs[\"labels\"] = labels[\"input_ids\"]\n  return model_inputs","metadata":{"id":"IdSY9Hun2WBh","execution":{"iopub.status.busy":"2024-06-17T05:02:02.776829Z","iopub.execute_input":"2024-06-17T05:02:02.777206Z","iopub.status.idle":"2024-06-17T05:02:02.855434Z","shell.execute_reply.started":"2024-06-17T05:02:02.777172Z","shell.execute_reply":"2024-06-17T05:02:02.854062Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True, batch_size = 256)\n\n# tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n# tokenized_test = small_test_dataset.map(preprocess_function, batched=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["13f4d50697524846ba6a4b93bced718d","1fceca26f4ea4634b0dc0bed312ca031","44f64bdbb6884c07bcab4deb69e1b0ca","d18ba62d1d674b40a3d03efd1c0a1318","794f7abdf8444480942f815a57fd0361","0258f3c200a14f239ec21e3837d3f374","438eeec7eeaa4d40bb95b71e652c96f8","ec92ff2e8f3544c2a0c045f26ce1d554","f4991483b0e14235b79479709a536f7b","1f0713bc90aa4d6ca275e4d85bc4234a","9ed2a08224f34e42a9a93f7cc68c5908"]},"id":"93uY6sSn1WMZ","outputId":"ab714ca8-1d08-45bc-8763-a1f11826b7bb","execution":{"iopub.status.busy":"2024-06-17T05:02:02.856842Z","iopub.execute_input":"2024-06-17T05:02:02.858087Z","iopub.status.idle":"2024-06-17T05:14:15.091169Z","shell.execute_reply.started":"2024-06-17T05:02:02.858047Z","shell.execute_reply":"2024-06-17T05:14:15.090204Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba11be86cfe44b6a06ac13e68900635"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c12671d571f456ba933f10c93a8b01b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db2050dd71247abb467e3b2ed7153bd"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset\n\n\n#tokenized_train","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ka2kFK995G52","outputId":"cebc8837-ea00-4ccf-f940-b32ad34b358e","execution":{"iopub.status.busy":"2024-06-17T05:14:15.092471Z","iopub.execute_input":"2024-06-17T05:14:15.092786Z","iopub.status.idle":"2024-06-17T05:14:15.098559Z","shell.execute_reply.started":"2024-06-17T05:14:15.092760Z","shell.execute_reply":"2024-06-17T05:14:15.097672Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'abstract', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 119924\n    })\n    validation: Dataset({\n        features: ['article', 'abstract', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 6633\n    })\n    test: Dataset({\n        features: ['article', 'abstract', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 6658\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine tune model","metadata":{"id":"M-GHYe0s5p1X"}},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AiyALwYQWaE","outputId":"6794dbd9-2544-4276-f1bd-5f556ff1dd91","execution":{"iopub.status.busy":"2024-06-17T05:14:15.099737Z","iopub.execute_input":"2024-06-17T05:14:15.100061Z","iopub.status.idle":"2024-06-17T05:14:31.707789Z","shell.execute_reply.started":"2024-06-17T05:14:15.100031Z","shell.execute_reply":"2024-06-17T05:14:31.706493Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=12177664bf8f9ffb27d68578b361cb1ea8aca3c5f8140320860e2d4e72381b29\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.tokenize import sent_tokenize\nimport evaluate\n\nrouge_score = evaluate.load(\"rouge\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Decode generated summaries into text\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Decode reference summaries into text\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # ROUGE expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n    # Compute ROUGE scores\n    result = rouge_score.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    # Extract the median scores\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"id":"WROXHEuTNi0h","execution":{"iopub.status.busy":"2024-06-17T05:14:31.710172Z","iopub.execute_input":"2024-06-17T05:14:31.710617Z","iopub.status.idle":"2024-06-17T05:14:32.814216Z","shell.execute_reply.started":"2024-06-17T05:14:31.710584Z","shell.execute_reply":"2024-06-17T05:14:32.813523Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b211dfd687f45e0bba99dd5539f040a"}},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:14:32.815327Z","iopub.execute_input":"2024-06-17T05:14:32.815594Z","iopub.status.idle":"2024-06-17T05:14:32.841181Z","shell.execute_reply.started":"2024-06-17T05:14:32.815570Z","shell.execute_reply":"2024-06-17T05:14:32.840396Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd727fe76e5746df8f7f5bc70db71a9b"}},"metadata":{}}]},{"cell_type":"code","source":"# import torch\n\n# # Enable CUDA_LAUNCH_BLOCKING\n# import os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:14:32.844333Z","iopub.execute_input":"2024-06-17T05:14:32.844598Z","iopub.status.idle":"2024-06-17T05:14:32.848125Z","shell.execute_reply.started":"2024-06-17T05:14:32.844575Z","shell.execute_reply":"2024-06-17T05:14:32.847328Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:14:32.849437Z","iopub.execute_input":"2024-06-17T05:14:32.849777Z","iopub.status.idle":"2024-06-17T05:14:32.861392Z","shell.execute_reply.started":"2024-06-17T05:14:32.849745Z","shell.execute_reply":"2024-06-17T05:14:32.860530Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    'bert-finetuning-cola',\n    eval_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.005,\n    save_total_limit=3,\n    num_train_epochs=1,\n    fp16=True,\n    gradient_accumulation_steps=4,\n    remove_unused_columns=False,\n    report_to = 'none'  # Equivalent to UPDATE_FREQ in other frameworks\n)\n\n# use the pre-built metrics \ndef compute_metrics(eval_preds):\n    metric = load_metric(\"glue\", \"cola\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Initialize the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=tokenized_dataset,\n#     eval_dataset=tokenized_dataset\n# )\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n)\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=small_train_dataset,\n#     eval_dataset=small_test_dataset,\n# )\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"0dPQHW4a5rVt","outputId":"6b445c53-0136-4742-8232-969004d16a2b","execution":{"iopub.status.busy":"2024-06-17T05:25:27.872717Z","iopub.execute_input":"2024-06-17T05:25:27.873091Z","iopub.status.idle":"2024-06-17T05:25:27.919387Z","shell.execute_reply.started":"2024-06-17T05:25:27.873062Z","shell.execute_reply":"2024-06-17T05:25:27.918542Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:25:31.207268Z","iopub.execute_input":"2024-06-17T05:25:31.207648Z","iopub.status.idle":"2024-06-17T08:44:27.996569Z","shell.execute_reply.started":"2024-06-17T05:25:31.207601Z","shell.execute_reply":"2024-06-17T08:44:27.995672Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3747' max='3747' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3747/3747 3:18:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.210700</td>\n      <td>1.998531</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3747, training_loss=2.3035198829636627, metrics={'train_runtime': 11936.2297, 'train_samples_per_second': 10.047, 'train_steps_per_second': 0.314, 'total_flos': 7.310984028880896e+16, 'train_loss': 2.3035198829636627, 'epoch': 0.9997998799279568})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Push to hub","metadata":{"id":"zgD9-5eFTQgJ"}},{"cell_type":"code","source":"","metadata":{"id":"wbyl5CVe8RSe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs = {\n    \"dataset_tags\": dataset_name,\n    \"dataset\": dataset_name,\n    \"model_name\": f\"{model_name}-pubmed\",\n    \"finetuned_from\": model_name,\n    \"tasks\": \"text-sumarization\",\n}","metadata":{"id":"gxEd7yrRR1Kj","execution":{"iopub.status.busy":"2024-06-17T08:49:54.999359Z","iopub.execute_input":"2024-06-17T08:49:54.999882Z","iopub.status.idle":"2024-06-17T08:49:55.006294Z","shell.execute_reply.started":"2024-06-17T08:49:54.999847Z","shell.execute_reply":"2024-06-17T08:49:55.004929Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(**kwargs)","metadata":{"id":"WC9kZr1GTUDJ","execution":{"iopub.status.busy":"2024-06-17T08:49:58.167745Z","iopub.execute_input":"2024-06-17T08:49:58.168658Z","iopub.status.idle":"2024-06-17T08:50:01.551222Z","shell.execute_reply.started":"2024-06-17T08:49:58.168605Z","shell.execute_reply":"2024-06-17T08:50:01.550319Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/QuanHcmus/bert-finetuning-cola/commit/a1186e35c122cbbfa470bf6947be351a58090b08', commit_message='End of training', commit_description='', oid='a1186e35c122cbbfa470bf6947be351a58090b08', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}